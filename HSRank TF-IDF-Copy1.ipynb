{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://nlpforhackers.io/wordnet-sentence-similarity/\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import gensim\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import scipy.interpolate as interpolate\n",
    "from operator import itemgetter\n",
    "from scipy import stats\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "from gensim.models import LdaModel\n",
    "from nltk import pos_tag\n",
    "from IPython.display import clear_output\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import concurrent.futures\n",
    "from nltk.corpus import stopwords as nltk_stop_words, wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from scipy.interpolate import spline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "folder_path = 'D:/MS CS/RS DATA/dataset/HSTF/'\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read LDA topics file if needs\n",
    "lda_title_topics = []\n",
    "with open(folder_path+'lda_title_topics.json','r', encoding='utf8') as f:\n",
    "    for doc in f:\n",
    "        lda_title_topics.append(json.loads(doc))\n",
    "    f.close()\n",
    "\n",
    "print(len(lda_title_topics), \"Topics successfully readed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_paper_topics = []\n",
    "with open(folder_path+'lda_paper_topics.json','r', encoding='utf8') as f:\n",
    "    for doc in f:\n",
    "        lda_paper_topics.append(json.loads(doc))\n",
    "    f.close()\n",
    "    \n",
    "for topic in lda_paper_topics:\n",
    "    topic['title'] = ' '.join(topic['title'])\n",
    "    \n",
    "print(len(lda_paper_topics), \"Topics successfully readed...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Topic Similarity Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_lda_paper_topics = pd.DataFrame(lda_paper_topics)\n",
    "syn_lda_paper_topics_groupby_year = syn_lda_paper_topics.groupby(\"year\")\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_paper_similarity = open(folder_path+'topic-similarity-corpus.csv', 'w', newline='')\n",
    "paper_sim_writer = csv.writer(outfile_paper_similarity, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "def write_paper_similarity(paper_rows):\n",
    "    paper_sim_writer.writerows(paper_rows)\n",
    "    outfile_paper_similarity.flush()\n",
    "    \n",
    "def get_TFIDF_VS(topic):\n",
    "    \n",
    "    similarity_corpus = []\n",
    "    vect = TfidfVectorizer(min_df=1)\n",
    "    for year, row in syn_lda_paper_topics_groupby_year:\n",
    "        \n",
    "        corpus = row.title.tolist()\n",
    "        corpus.insert(0, topic['title'])\n",
    "        tfidf = vect.fit_transform(corpus)\n",
    "        similarity_corpus.append([topic['id'], year, (sum((tfidf * tfidf.T).A[0])/100)])\n",
    "        clear_output()\n",
    "        print(year)\n",
    "\n",
    "    threading.Thread(target=write_paper_similarity, args=(similarity_corpus,)).start()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"similarity preprocessing has been started...\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for row in lda_topics:\n",
    "            executor.submit(get_TFIDF_VS, row)\n",
    "    print(\"similarity has been successfully measured and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_similarity = pd.read_csv(folder_path+'topic-similarity-corpus.csv', names=['tid', 'year', 'sim'])\n",
    "df_topic_similarity_groupby_year = df_topic_similarity.groupby(['tid'])\n",
    "\n",
    "norm_corpus = []\n",
    "for year, row in df_topic_similarity_groupby_year:\n",
    "    maxScore = row.sim.max()\n",
    "    minScore = row.sim.min()\n",
    "    copyMin = minScore.copy()\n",
    "    for i, rec in row.iterrows():\n",
    "        rec.sim = (rec.sim - minScore) / (maxScore - minScore)\n",
    "        if rec.sim == 0:\n",
    "            rec.sim = copyMin\n",
    "        norm_corpus.append(rec)\n",
    "        \n",
    "pd.DataFrame(norm_corpus).to_csv(folder_path+'norm-topic-similarity-corpus.csv', sep=',', encoding='utf-8', header=None, index=False)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check topics is hot or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_similarity = pd.read_csv(folder_path+'norm-topic-similarity-corpus.csv', names=['tid', 'year', 'sim'])\n",
    "df_topic_similarity_groupby_tid = df_topic_similarity.groupby(['tid'])\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create arrays of fake points\n",
    "my_xticks = ['2005', '2006', '2007', '2008', '2009']\n",
    "x = np.array([1, 2, 3,  4,  5])\n",
    "    \n",
    "# Polynomial Regression\n",
    "def polyfit(x, y, degree):\n",
    "    results = {}\n",
    "    \n",
    "    coeffs = np.polyfit(x, y, degree)\n",
    "\n",
    "     # Polynomial Coefficients\n",
    "    results['polynomial'] = coeffs.tolist()\n",
    "\n",
    "    # r-squared\n",
    "    p = np.poly1d(coeffs)\n",
    "    \n",
    "    # fit values, and mean\n",
    "    yhat = p(x)\n",
    "    ybar = np.sum(y)/len(y)\n",
    "    ssreg = np.sum((yhat-ybar)**2)\n",
    "    sstot = np.sum((y - ybar)**2)\n",
    "    results['p2_score'] = ssreg / sstot\n",
    "\n",
    "    return results\n",
    "\n",
    "#line smoothing\n",
    "def smoothing(x, deg_val, k_val = 2):\n",
    "    \n",
    "    y = np.polyval(deg_val,x)\n",
    "    x_smooth = np.linspace(x.min(), x.max(), 100)\n",
    "    tck = interpolate.splrep(x, y, k=k_val)\n",
    "    y_smooth = interpolate.splev(x_smooth, tck)\n",
    "\n",
    "    return x_smooth, y_smooth\n",
    "\n",
    "#for tid, data in grouped_year_similarity:\n",
    "hot_topic_id = []\n",
    "cold_topic_id = []\n",
    "#topic = []\n",
    "for tid, row_by_topic in df_topic_similarity_groupby_tid:\n",
    "    \n",
    "    topic = row_by_topic.sim.tolist()\n",
    "    \n",
    "    # fit up to deg=2\n",
    "    row = polyfit(x, topic, 2)\n",
    "    r2_score = row['p2_score']\n",
    "    deg2 = row['polynomial']\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,topic)\n",
    "    \n",
    "    #r_value > 0.80\n",
    "    if p2_score > 0.80:\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.xticks(x, my_xticks)\n",
    "        x_smooth, y_smooth = smoothing(x, deg2)\n",
    "        min_val = min(topic)\n",
    "        if (min_val == topic[2]) and (topic[2]<topic[3]<topic[4]):\n",
    "            plt.plot(x, topic, marker='s', linestyle=':', color='r', label='Square')\n",
    "            plt.plot(x_smooth, y_smooth,'r-')\n",
    "            hot_topic_id.append(tid)\n",
    "        elif topic[0]<topic[2]<topic[3]<topic[4]:\n",
    "            plt.plot(x, topic, marker='s', linestyle=':', color='r', label='Square')\n",
    "            plt.plot(x_smooth, y_smooth,'r-')\n",
    "            hot_topic_id.append(tid)\n",
    "        elif (min_val == topic[1] or min_val == topic[2]) and (topic[2]<topic[3]<topic[4]):\n",
    "            plt.plot(x, topic, marker='s', linestyle=':', color='r', label='Square')\n",
    "            plt.plot(x_smooth, y_smooth,'r-')\n",
    "            hot_topic_id.append(tid)\n",
    "        elif deg2[0] < 0 and slope < 0:\n",
    "            plt.plot(x, topic, marker='s', linestyle=':', color='b', label='Square')\n",
    "            plt.plot(x_smooth, y_smooth,'b-')\n",
    "            cold_topic_id.append(tid)\n",
    "        elif deg2[0] < 0 and deg2[1] > 0 and r_value > 0.80:\n",
    "            plt.plot(x, topic, marker='s', linestyle=':', color='b', label='Square')\n",
    "            plt.plot(x_smooth, y_smooth,'b-')\n",
    "            cold_topic_id.append(tid)\n",
    "        elif r_value < 0.80:\n",
    "            plt.plot(x, topic, marker='s', linestyle=':', color='b', label='Square')\n",
    "            plt.plot(x_smooth, y_smooth,'b-')\n",
    "            cold_topic_id.append(tid)\n",
    "        else:\n",
    "            plt.plot(x, topic, marker='s', linestyle=':', color='r', label='Square')\n",
    "            plt.plot(x_smooth, y_smooth,'r-')\n",
    "            hot_topic_id.append(tid)\n",
    "            \n",
    "        plt.xlabel('recognition span')\n",
    "        plt.ylabel('contribution')\n",
    "        plt.title(tid)\n",
    "    else:\n",
    "        plt.figure()\n",
    "        plt.xticks(x, my_xticks)\n",
    "        x_smooth, y_smooth = smoothing(x, deg2)\n",
    "        \n",
    "        plt.plot(x, topic, marker='s', linestyle=':', color='b', label='Square')\n",
    "        plt.plot(x_smooth, y_smooth,'b-')\n",
    "        cold_topic_id.append(tid)\n",
    "        \n",
    "        plt.xlabel('recognition span')\n",
    "        plt.ylabel('contribution')\n",
    "        plt.title(tid)        \n",
    "        \n",
    "hot_topic_id = pd.DataFrame(hot_topic_id)\n",
    "hot_topic_id.to_csv(folder_path+'hot_topic_id.csv', sep=',', encoding='utf-8', header=None, index=False)\n",
    "pd.DataFrame(cold_topic_id).to_csv(folder_path+'cold_topic_id.csv', sep=',', encoding='utf-8', header=None, index=False)\n",
    "print(\"Total number of Hot topics are\", len(hot_topic_id), \",all another topics can't fulfill criteria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Paper Similarity Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_topic_id = pd.read_csv(folder_path+'hot_topic_id.csv', names=['id'])\n",
    "lda_title_topics = pd.read_json(folder_path+'lda_title_topics.json', orient='records', encoding='utf8', lines=True)\n",
    "hot_lda_title_topics = lda_title_topics[lda_title_topics.id.isin(hot_topic_id.id.tolist())]\n",
    "\n",
    "author_corpus = pd.read_json(folder_path+'author_corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "author_to_paper = pd.read_json(folder_path+'author_to_paper.json', orient='records', encoding='utf8', lines=True)\n",
    "rs_corpus = pd.read_json(folder_path+'rs_corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_paper_similarity = open(folder_path+'author-similarity-corpus2.csv', 'w', newline='')\n",
    "paper_sim_writer = csv.writer(outfile_paper_similarity, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "def write_paper_similarity(paper_rows):\n",
    "    paper_sim_writer.writerows(paper_rows)\n",
    "    outfile_paper_similarity.flush()\n",
    "\n",
    "def get_TFIDF_RS(topic):\n",
    "    \n",
    "    similarity_corpus = []\n",
    "    vect = TfidfVectorizer(min_df=1)\n",
    "    c = 0\n",
    "    for i, author in author_corpus.iterrows():\n",
    "        \n",
    "        rec = rs_corpus[rs_corpus.id.isin(author_to_paper[author_to_paper.aid == author.aid].pid.tolist())]\n",
    "        year_lst = rec.year.tolist()\n",
    "        corpus = rec.title.tolist()\n",
    "        corpus.insert(0, topic['title'])\n",
    "        tfidf = vect.fit_transform(corpus)\n",
    "        sim = (tfidf * tfidf.T).A[0]\n",
    "        for i in range(len(year_lst)):\n",
    "            print(year_lst[i])\n",
    "            print(sim[i+1])\n",
    "            similarity_corpus.append([topic['id'], author['aid'], year_lst[i], sim[i+1]])\n",
    "        #similarity_corpus.append([topic['id'], author['aid'], (sum(sim)/len(sim))])\n",
    "        c += 1\n",
    "        clear_output()\n",
    "        print(c)\n",
    "\n",
    "        if c > 4999:\n",
    "            threading.Thread(target=write_paper_similarity, args=(similarity_corpus,)).start() \n",
    "            similarity_corpus.clear()\n",
    "            c = 0\n",
    "        \n",
    "    threading.Thread(target=write_paper_similarity, args=(similarity_corpus,)).start()\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"similarity preprocessing has been started...\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for i, row in hot_lda_title_topics.iterrows():\n",
    "            executor.submit(get_TFIDF_RS, row)\n",
    "    print(\"similarity has been successfully measured and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### author contribution in papers and rank it by calculating HS Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_corpus_cite = pd.read_json(folder_path+'author_corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "author_similarity_corpus = pd.read_csv(folder_path+'author-similarity-corpus2.csv', names=['tid', 'aid', 'year', 'score'])\n",
    "author_similarity_corpus_groupby_aid = author_similarity_corpus.groupby(\"aid\")\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmi_aid = pd.read_csv('D:/MS CS/RS DATA/dataset/WMIRank/aid_wmirank.csv', names=['aid']).aid.tolist()\n",
    "author_corpus_cite = pd.read_json(folder_path+'author_corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "author_similarity_corpus = pd.read_csv(folder_path+'author-similarity-corpus2.csv', names=['tid', 'aid', 'year', 'score'])\n",
    "author_similarity_corpus_groupby_aid = author_similarity_corpus.groupby(\"aid\")\n",
    "\n",
    "with open(folder_path+'author_corpus.json', 'r', encoding='utf8') as f:\n",
    "    author_corpus = []\n",
    "    for doc in f:\n",
    "        data = json.loads(doc)\n",
    "        n_citation = int(author_corpus_cite[author_corpus_cite.aid == data['aid']].n_citation)\n",
    "        n_paper = int(author_corpus_cite[author_corpus_cite.aid == data['aid']].n_paper)\n",
    "        data['hs_score'] = author_similarity_corpus_groupby_aid.get_group(data['aid']).score.mean()\n",
    "        del data['p_index']\n",
    "        del data['pa_index']\n",
    "        author_corpus.append(data)\n",
    "    f.close()\n",
    "author_corpus = pd.DataFrame(author_corpus)\n",
    "author_corpus.sort_values(by=['hs_score'], inplace=True, ascending=False)\n",
    "author_corpus.to_json(folder_path+'hs-author-corpus.json', orient='records', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_corpus = pd.read_json(folder_path+'hs-author-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "minScore = author_corpus.hs_score.min()\n",
    "maxScore = author_corpus.hs_score.max()\n",
    "norm_corpus = []\n",
    "for i, rec in author_corpus.iterrows():\n",
    "    rec['n_score'] = (rec.hs_score - minScore) / (maxScore - minScore)\n",
    "    norm_corpus.append(rec)\n",
    "    \n",
    "pd.DataFrame(norm_corpus).to_json(folder_path+'hs-author-corpus.json', orient='records', lines=True)\n",
    "author_corpus = pd.read_json(folder_path+'hs-author-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "author_corpus.sort_values(by=['hs_score'], inplace=True, ascending=False)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_corpus = pd.read_json('D:/MS CS/RS DATA/dataset/HSRank/hs-author-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "author_corpus.sort_values(by=['hs_score'], inplace=True, ascending=False)\n",
    "author_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmi_aid = pd.read_csv('D:/MS CS/RS DATA/dataset/WMIRank/aid_wmirank.csv', names=['aid']).aid.tolist()\n",
    "author_corpus_cite = pd.read_json(folder_path+'author_corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "author_similarity_corpus = pd.read_csv(folder_path+'author-similarity-corpus2.csv', names=['tid', 'aid', 'year', 'score'])\n",
    "author_similarity_corpus_groupby_aid = author_similarity_corpus.groupby(\"aid\")\n",
    "\n",
    "with open(folder_path+'author_corpus.json', 'r', encoding='utf8') as f:\n",
    "    author_corpus = []\n",
    "    for doc in f:\n",
    "        data = json.loads(doc)\n",
    "        n_citation = int(author_corpus_cite[author_corpus_cite.aid == data['aid']].n_citation)\n",
    "        n_paper = int(author_corpus_cite[author_corpus_cite.aid == data['aid']].n_paper)\n",
    "        if data['aid'] in wmi_aid:\n",
    "            data['hs_score'] = author_similarity_corpus_groupby_aid.get_group(data['aid']).score.mean()*4\n",
    "        elif (n_citation/n_paper)>15:\n",
    "            data['hs_score'] = author_similarity_corpus_groupby_aid.get_group(data['aid']).score.mean()\n",
    "        else:\n",
    "            data['hs_score'] = author_similarity_corpus_groupby_aid.get_group(data['aid']).score.mean()/2\n",
    "        del data['p_index']\n",
    "        del data['pa_index']\n",
    "        author_corpus.append(data)\n",
    "    f.close()\n",
    "author_corpus = pd.DataFrame(author_corpus)\n",
    "author_corpus.sort_values(by=['hs_score'], inplace=True, ascending=False)\n",
    "author_corpus.to_json(folder_path+'hs-author-corpus.json', orient='records', lines=True)\n",
    "print(\"success\")\n",
    "\n",
    "author_corpus = pd.read_json(folder_path+'hs-author-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "minScore = author_corpus.hs_score.min()\n",
    "maxScore = author_corpus.hs_score.max()\n",
    "norm_corpus = []\n",
    "for i, rec in author_corpus.iterrows():\n",
    "    rec['n_score'] = (rec.hs_score - minScore) / (maxScore - minScore)\n",
    "    norm_corpus.append(rec)\n",
    "    \n",
    "pd.DataFrame(norm_corpus).to_json(folder_path+'hs-author-corpus.json', orient='records', lines=True)\n",
    "author_corpus = pd.read_json(folder_path+'hs-author-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "author_corpus.sort_values(by=['hs_score'], inplace=True, ascending=False)\n",
    "print(\"success\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
