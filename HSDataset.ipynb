{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://aminer.org/aminernetwork\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "#import ijson\n",
    "import bisect\n",
    "#import gensim\n",
    "import nltk\n",
    "import _thread\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import scipy.interpolate as interpolate\n",
    "from operator import itemgetter\n",
    "from scipy import stats\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "#from gensim.models import LdaModel\n",
    "from stop_words import get_stop_words\n",
    "from nltk import pos_tag\n",
    "from IPython.display import clear_output\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from nltk.corpus import stopwords as nltk_stop_words, wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from scipy.interpolate import spline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "folder_path = 'D:/MS CS/RS DATA/dataset/'\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> Just Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    STEXT = 'Note:- If you want to '\n",
    "    ETEXT = ', then run next cell otherwise ignore it...'\n",
    "    EETEXT = ' otherwise ignore it...'\n",
    "\n",
    "clear_output()\n",
    "#default number of papers\n",
    "number_of_papers = 0\n",
    "\n",
    "#just file name\n",
    "file_number = 4\n",
    "\n",
    "# path where files reside\n",
    "folder_path = 'D:/MS CS/RS DATA/dataset/backup/'\n",
    "#folder_path = 'E:/MS CS/RS DATA/dblp-ref/'\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "# Bring in the default English NLTK stop words\n",
    "en_stop += nltk_stop_words.words('english')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "print(\"All variables have initialized...\")\n",
    "print(bcolors.STEXT+bcolors.HEADER+\"perform pre-precessing on new dataset\"+bcolors.ENDC+bcolors.ETEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_of_paper = pd.read_csv(folder_path+'cite.csv', sep=',', names=['pid', 'citation'])\n",
    "rs_corpus = []\n",
    "with open(folder_path+'rs-corpus.json','r', encoding=\"utf8\") as f:\n",
    "    for doc in f:\n",
    "        rs_corpus.append(json.loads(doc))\n",
    "    f.close()\n",
    "print(\"dataset loaded...!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in rs_corpus:\n",
    "    citation = list(citation_of_paper[citation_of_paper['pid']==doc['id']]['citation'])\n",
    "    if len(citation)>0:\n",
    "        doc['citation'] = int(citation[0])\n",
    "    else:\n",
    "        doc['citation'] = 0\n",
    "pd.DataFrame(rs_corpus).to_json(folder_path+'rs-corpus.json', orient='records', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on papers corpus\n",
    "#### -> convert text to JSON file (RS and VS corpus)\n",
    "#### -> tokenization\n",
    "#### -> stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for tokenized documents in loop\n",
    "#file_rs = open(folder_path+'rs-corpus.json', 'w+', encoding=\"utf8\")\n",
    "#file_vs = open(folder_path+'vs-corpus.json', 'w+', encoding=\"utf8\")\n",
    "\n",
    "def topic_preprocessing(data_topic):\n",
    "    # clean and tokenize document string\n",
    "    raw = (data_topic.lower().strip(':-;?!.,@')).strip()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if i not in en_stop]\n",
    "\n",
    "    # steme words from stopped_tokens\n",
    "    stemmed_tokens = []\n",
    "    for i in stopped_tokens:\n",
    "        stm = p_stemmer.stem(i.strip())\n",
    "        if len(stm)>1 and re.match('^[a-zA-Z0-9_]+$', stm) and not stm.isdigit():\n",
    "            tag = wn.synsets(stm)\n",
    "            if not tag:\n",
    "                stemmed_tokens.append(stm)\n",
    "            if tag:\n",
    "                pos = tag[0].pos()\n",
    "                if pos != 'v':\n",
    "                    stemmed_tokens.append(stm)\n",
    "            \n",
    "    return stemmed_tokens\n",
    "\n",
    "def add_doc_to_rs(rs_data):\n",
    "    #clear_output()\n",
    "    #print(rs_data['title'])\n",
    "    #print(rs_data['abstract'])\n",
    "    #rs_data['title'] = topic_preprocessing(rs_data['title'])\n",
    "    #rs_data['abstract'] = topic_preprocessing(rs_data['abstract'])\n",
    "    file_rs.write(json.dumps(rs_data)+'\\n')\n",
    "    \n",
    "def add_doc_to_vs(vs_data):\n",
    "    file_vs.write(json.dumps(vs_data)+'\\n')\n",
    "\n",
    "def init_param():\n",
    "    global data\n",
    "    data = {}\n",
    "    global title\n",
    "    title = ''\n",
    "    global abstract\n",
    "    abstract = ''\n",
    "    global pub_year\n",
    "    pub_year = 1000\n",
    "    global references\n",
    "    references = []\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    doc_set_rs = []\n",
    "    doc_set_vs = []\n",
    "    #pool = ThreadPool(4)\n",
    "    count = 0\n",
    "    with open(folder_path+'AMiner-Paper.txt', 'r', encoding=\"utf8\") as f:\n",
    "        init_param()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith( '#index' ):\n",
    "                data['id'] = int((line.split('#index')[1]).strip())\n",
    "            elif line.startswith( '#*' ):\n",
    "                title = (line.split('#*')[1]).strip()\n",
    "            elif line.startswith( '#@' ):\n",
    "                data['authors'] = [a.strip() for a in (line.split('#@')[1]).split(';')]\n",
    "            elif line.startswith( '#o' ):\n",
    "                data['affiliations'] = [a.strip() for a in (line.split('#o')[1]).split(';')]\n",
    "            elif line.startswith( '#t' ):\n",
    "                year = (line.split('#t')[1]).strip()\n",
    "                if not year or year == \"\":\n",
    "                    data['year'] = 0\n",
    "                else:\n",
    "                    pub_year = int(year)\n",
    "                    data['year'] = pub_year\n",
    "            elif line.startswith( '#c' ):\n",
    "                data['venue'] = (line.split('#c')[1]).strip()\n",
    "            elif line.startswith( '#%' ):\n",
    "                references.append(int((line.split('#%')[1]).strip()))\n",
    "            elif line.startswith( '#!' ):\n",
    "                abstract = (line.split('#!')[1]).strip()\n",
    "            else:\n",
    "                count += 1\n",
    "                if count % 100000 == 0:\n",
    "                    print(count, \"documents selected...\")\n",
    "                if not line or line == \"\":\n",
    "                    if not data.keys():\n",
    "                        init_param()\n",
    "                    else:\n",
    "                        data['references'] = references\n",
    "                        if pub_year > 2009:\n",
    "                            if title:\n",
    "                                data['title'] = title\n",
    "                                doc_set_vs.append(data)\n",
    "                        elif pub_year > 2004:\n",
    "                            if title:\n",
    "                                data['title'] = title\n",
    "                                doc_set_rs.append(data)\n",
    "                        init_param()\n",
    "        \n",
    "        clear_output()                        \n",
    "        print(count, \"documents selected...\")                        \n",
    "        print(\"Data preprocessing has been started...\")\n",
    "        # and return the results\n",
    "        #for data in doc_set_rs:\n",
    "            #add_doc_to_rs(data)\n",
    "        #pool.map(add_doc_to_rs, doc_set_rs)\n",
    "        #pool.map(add_doc_to_vs, doc_set_vs)\n",
    "        pd.DataFrame(doc_set_vs).to_json(folder_path+'doc_set_vs.json', orient='records', lines=True)\n",
    "        pd.DataFrame(doc_set_rs).to_json(folder_path+'doc_set_rs.json', orient='records', lines=True)\n",
    "        # close the pool and wait for the work to finish \n",
    "        #pool.close()\n",
    "        #pool.join()\n",
    "        #file_rs.close()\n",
    "        #file_vs.close()\n",
    "        #clear_output()\n",
    "        print(\"Data preprocessing has been done and saved...\")\n",
    "        print(bcolors.STEXT+bcolors.HEADER+\"pre-process authors doc-set\"+bcolors.ENDC+bcolors.ETEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_set_rs[doc_set_rs.id.isin(author_to_paper.pid.tolist())].to_json(folder_path+'doc_set_rs2.json', orient='records', lines=True)\n",
    "doc_set_vs[doc_set_vs.id.isin(vs_corpus_avg.id.tolist())].to_json(folder_path+'doc_set_vs2.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_set_rs = pd.read_json(folder_path+'doc_set_rs2.json', orient='records', encoding='utf8', lines=True)\n",
    "doc_set_vs = pd.read_json(folder_path+'doc_set_vs2.json', orient='records', encoding='utf8', lines=True)\n",
    "#vs_corpus_avg = pd.read_json(folder_path+'vs_corpus_avg.json', orient='records', encoding='utf8', lines=True)\n",
    "#author_to_paper = pd.read_json(folder_path+'author_to_paper.json', orient='records', encoding='utf8', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_preprocessing(data_topic):\n",
    "    # clean and tokenize document string\n",
    "    raw = (data_topic.lower().strip(':-;?!.,@')).strip()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens     \n",
    "    return [i for i in tokens if i not in en_stop and len(i)>1 and re.match('^[a-zA-Z0-9_]+$', i) and not i.isdigit()]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    set_rs = []\n",
    "    set_vs = []\n",
    "    for k, v in doc_set_rs.iterrows():\n",
    "        v.title = topic_preprocessing(v.title)\n",
    "        set_rs.append(v)\n",
    "    \n",
    "    for k, v in doc_set_vs.iterrows():\n",
    "        v.title = topic_preprocessing(v.title)\n",
    "        set_vs.append(v)\n",
    "        \n",
    "    pd.DataFrame(set_vs).to_json(folder_path+'set_vs.json', orient='records', lines=True)\n",
    "    pd.DataFrame(set_rs).to_json(folder_path+'set_rs.json', orient='records', lines=True)\n",
    "    print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_vs = open(folder_path+'vs-corpus-reff.json', 'w+', encoding=\"utf8\")\n",
    "\n",
    "def topic_preprocessing(data_topic):\n",
    "    # clean and tokenize document string\n",
    "    raw = (data_topic.lower().strip(':-;?!.,@')).strip()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if i not in en_stop]\n",
    "\n",
    "    # steme words from stopped_tokens\n",
    "    stemmed_tokens = []\n",
    "    for i in stopped_tokens:\n",
    "        stm = p_stemmer.stem(i.strip())\n",
    "        if len(stm)>1 and re.match('^[a-zA-Z0-9_]+$', stm) and not stm.isdigit():\n",
    "            tag = wn.synsets(stm)\n",
    "            if not tag:\n",
    "                stemmed_tokens.append(stm)\n",
    "            if tag:\n",
    "                pos = tag[0].pos()\n",
    "                if pos != 'v':\n",
    "                    stemmed_tokens.append(stm)\n",
    "            \n",
    "    return stemmed_tokens\n",
    "    \n",
    "def add_doc_to_vs(vs_data):\n",
    "    #clear_output()\n",
    "    #print(vs_data['title'])\n",
    "    #vs_data['title'] = topic_preprocessing(vs_data['title'])\n",
    "    file_vs.write(json.dumps(vs_data)+'\\n')\n",
    "\n",
    "def init_param():\n",
    "    global data\n",
    "    data = {}\n",
    "    global title\n",
    "    title = ''\n",
    "    global abstract\n",
    "    abstract = ''\n",
    "    global pub_year\n",
    "    pub_year = 1000\n",
    "    global references\n",
    "    references = []\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    doc_set_rs = []\n",
    "    doc_set_vs = []\n",
    "    #pool = ThreadPool(4)\n",
    "    count = 0\n",
    "    with open(folder_path+'backup/AMiner-Paper.txt', 'r', encoding=\"utf8\") as f:\n",
    "        init_param()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith( '#index' ):\n",
    "                data['id'] = int((line.split('#index')[1]).strip())\n",
    "            elif line.startswith( '#*' ):\n",
    "                title = (line.split('#*')[1]).strip()\n",
    "            elif line.startswith( '#@' ):\n",
    "                temp = 1\n",
    "                #data['authors'] = [a.strip() for a in (line.split('#@')[1]).split(';')]\n",
    "            elif line.startswith( '#o' ):\n",
    "                temp = 1\n",
    "                #data['affiliations'] = [a.strip() for a in (line.split('#o')[1]).split(';')]\n",
    "            elif line.startswith( '#t' ):\n",
    "                year = (line.split('#t')[1]).strip()\n",
    "                if not year or year == \"\":\n",
    "                    data['year'] = 0\n",
    "                else:\n",
    "                    pub_year = int(year)\n",
    "                    data['year'] = pub_year\n",
    "            elif line.startswith( '#c' ):\n",
    "                temp = 1\n",
    "                #data['venue'] = (line.split('#c')[1]).strip()\n",
    "            elif line.startswith( '#%' ):\n",
    "                references.append(int((line.split('#%')[1]).strip()))\n",
    "            elif line.startswith( '#!' ):\n",
    "                temp = 1\n",
    "                #abstract = (line.split('#!')[1]).strip()\n",
    "            else:\n",
    "                count += 1\n",
    "                if count % 100000 == 0:\n",
    "                    print(count, \"documents selected...\")\n",
    "                if not line or line == \"\":\n",
    "                    if not data.keys():\n",
    "                        init_param()\n",
    "                    else:\n",
    "                        if (pub_year > 2009) and len(references)>0:\n",
    "                            data['references'] = references\n",
    "                            #data['title'] = title\n",
    "                            doc_set_vs.append(data)\n",
    "                        init_param()\n",
    "                        \n",
    "        \n",
    "        clear_output()                        \n",
    "        print(count, \"documents selected...\")                        \n",
    "        print(\"Data preprocessing has been started...\")\n",
    "        # and return the results\n",
    "        for data in doc_set_vs:\n",
    "            add_doc_to_vs(data)\n",
    "        file_vs.close()\n",
    "        clear_output()\n",
    "        print(\"Data preprocessing has been done and saved...\")\n",
    "        print(bcolors.STEXT+bcolors.HEADER+\"pre-process authors doc-set\"+bcolors.ENDC+bcolors.ETEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 0\n",
    "with open(folder_path+'backup/AMiner-Paper.txt', 'r', encoding=\"utf8\") as f:\n",
    "\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith( '#index' ):\n",
    "            id = int((line.split('#index')[1]).strip())\n",
    "        elif line.startswith( '#*' ):\n",
    "            title = (line.split('#*')[1]).strip()\n",
    "            if id == 2077796:\n",
    "                print(title)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_corpus = []\n",
    "with open(folder_path+'rs-corpus.json','r', encoding=\"utf8\") as f:\n",
    "    for doc in f:\n",
    "        rs_corpus.append(json.loads(doc))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on Author-2-Paper\n",
    "#### -> Links of senior authors removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_author_pid = []\n",
    "with open(folder_path+'vs-corpus.json','r', encoding=\"utf8\") as f:\n",
    "    for doc in f:\n",
    "        senior_author_pid.append(json.loads(doc)['id'])\n",
    "    f.close()\n",
    "    \n",
    "author_to_paper = pd.read_csv(folder_path+'AMiner-Author2Paper.txt', sep='\\t', names=['aid', 'pid', 'pos'])\n",
    "print(\"started\")\n",
    "senior_author = author_to_paper[author_to_paper['pid'].isin(senior_author_pid)]\n",
    "senior_author_pid = list(set(senior_author['pid']))\n",
    "author_to_paper = author_to_paper[~author_to_paper['pid'].isin(senior_author_pid)]\n",
    "author_to_paper.to_json(folder_path+'author-2-paper.json', orient='records', lines=True)\n",
    "pd.DataFrame(senior_author, columns=['aid', 'pid', 'pos']).to_csv(folder_path+'senior_author.csv', sep=',', encoding='utf-8', index=False)\n",
    "print(\"success\")\n",
    "print(len(author_to_paper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_to_paper = pd.read_csv(folder_path+'AMiner-Author2Paper.txt', sep='\\t', names=['aid', 'pid', 'pos'])\n",
    "paper_id = author_to_paper[(author_to_paper.aid.isin(author.aid)) & (author_to_paper.pos == 1)].pid\n",
    "author_to_paper = author_to_paper[author_to_paper.pid.isin(paper_id)]\n",
    "author_to_paper.to_json(folder_path+'author-2-paper.json', orient='records', lines=True)\n",
    "rs_corpus = pd.read_json(folder_path+'rs-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "rs_corpus[rs_corpus.id.isin(paper_id)]\n",
    "rs_corpus.to_json(folder_path+'rs-corpus.json', orient='records', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#author_to_paper[(author_to_paper.aid==481344) & (author_to_paper.pos==1)]\n",
    "#author_to_paper = pd.read_json(folder_path+'author-2-paper.json', orient='records', encoding='utf8', lines=True)\n",
    "senior_author = pd.read_csv(folder_path+'senior_author.csv', sep=',', names=['aid', 'pid', 'pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_author_to_paper = pd.read_json(folder_path+'vs-author-2-paper.json', orient='records', encoding='utf8', lines=True)\n",
    "vs_corpus = pd.read_json(folder_path+'vs-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "grouped_vs_corpus = vs_corpus.groupby(\"year\")\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = []\n",
    "for year, data in grouped_vs_corpus:\n",
    "    for k, v in vs_author_to_paper[(vs_author_to_paper.pid.isin(data.id)) & (vs_author_to_paper.pos==1)].groupby(\"aid\"):\n",
    "        if v.pid.count()<3:\n",
    "            pid.append(v.pid.tolist())\n",
    "            \n",
    "from itertools import chain\n",
    "pid = list(chain.from_iterable(pid))\n",
    "print(len(pid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VS Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_corpus_reff = pd.read_json(folder_path+'vs-corpus-reff.json', orient='records', encoding='utf8', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_vs_corpus_reff = vs_corpus_reff.groupby(\"year\")\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_ref_set = []\n",
    "for k, v in grouped_vs_corpus_reff:\n",
    "    \n",
    "    year_ref = {}\n",
    "    ref = []\n",
    "    for reff in v.references:\n",
    "        ref.append(len(reff))\n",
    "    year_ref['year'] = k\n",
    "    year_ref['references'] = ref\n",
    "    year_ref[\"total_paper\"] = v.references.count()\n",
    "    year_ref_set.append(year_ref)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "reff_set = []\n",
    "for v in year_ref_set:\n",
    "    \n",
    "    data = {}\n",
    "    data[\"year\"] = v['year']\n",
    "    data[\"avg\"] = (sum(v['references'])/len(v['references']))\n",
    "    data[\"stdev\"] = statistics.stdev(v['references'])\n",
    "    reff_set.append(data)\n",
    "    \n",
    "    print(\"year:\", data[\"year\"])\n",
    "    print(\"avg\", data[\"avg\"])\n",
    "    print(\"stdev\", data[\"stdev\"])\n",
    "    print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdev_pid =[]\n",
    "avg_pid = []\n",
    "for row in reff_set:\n",
    "    for k, paper in vs_corpus_reff[(vs_corpus_reff.year == row['year'])].iterrows():\n",
    "        \n",
    "        if len(paper.references)<row['stdev']:\n",
    "            stdev_pid.append(paper.id)\n",
    "        if paper.year == 2014:\n",
    "            if len(paper.references)<25:\n",
    "                avg_pid.append(paper.id)\n",
    "        elif len(paper.references)<33:\n",
    "            avg_pid.append(paper.id)\n",
    "            \n",
    "print(\"stdev_pid\", len(stdev_pid))\n",
    "print(\"avg_pid\", len(avg_pid))\n",
    "print(\"success\")\n",
    "\n",
    "g_dummy = vs_corpus_reff[~vs_corpus_reff.id.isin(avg_pid)]\n",
    "#g_dummy = g_dummy[~g_dummy.id.isin(avg_pid)]\n",
    "\n",
    "for year, v in g_dummy.groupby('year'):\n",
    "    print(year)\n",
    "    print(len(v))\n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_similarity_corpus = pd.read_csv(folder_path+'vs-topic-paper-similarity-corpus.csv', names=['tid', 'pid', 'year', 'sim'])\n",
    "for year, v in vs_similarity_corpus.groupby('year'):\n",
    "    print(year)\n",
    "    print(len(v))\n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_corpus_reff = vs_corpus_reff[~vs_corpus_reff.id.isin(avg_pid)]\n",
    "g_dummy = vs_corpus_reff[~vs_corpus_reff.id.isin(avg_pid)]\n",
    "\n",
    "for year, v in g_dummy.groupby('year'):\n",
    "    print(year)\n",
    "    print(len(v))\n",
    "    print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_corpus = pd.read_json(folder_path+'vs_corpus_avg.json', orient='records', encoding='utf8', lines=True)\n",
    "vs_corpus = vs_corpus[vs_corpus.id.isin(vs_corpus_reff.id.tolist())]\n",
    "vs_corpus.to_json(folder_path+'vs_corpus_avg.json', orient='records', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_similarity_corpus = pd.read_csv(folder_path+'complete_vs_similarity_corpus.csv', names=['tid', 'pid', 'year', 'sim'])\n",
    "vs_similarity_corpus = vs_similarity_corpus[vs_similarity_corpus.pid.isin(vs_corpus.id)]\n",
    "vs_similarity_corpus.to_csv(folder_path+'vs_similarity_corpus.csv', sep=',', encoding='utf-8', index=False)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on author corpus\n",
    "#### -> Just convert text - to - JSON file\n",
    "#### -> Select authors (Juniors & Seniors), who publish paper in RS\n",
    "#### -> Remove authors, who havn't affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "author_corpus = []\n",
    "with open(folder_path+'AMiner-Author.txt', 'r', encoding=\"utf8\") as f:\n",
    "\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith( '#index' ):\n",
    "            data['aid'] = int((line.split('#index')[1]).strip())\n",
    "        elif line.startswith( '#n' ):\n",
    "            data['authors'] = [a.strip() for a in (line.split('#n')[1]).split(';')]\n",
    "        elif line.startswith( '#a' ):\n",
    "            data['affiliations'] = [a.strip() for a in (line.split('#a')[1]).split(';')]\n",
    "        elif line.startswith( '#pc' ):\n",
    "            data['n_paper'] = int((line.split('#pc')[1]).strip())\n",
    "        elif line.startswith( '#cn' ):\n",
    "            data['n_citation'] = int((line.split('#cn')[1]).strip())\n",
    "        elif line.startswith( '#hi' ):\n",
    "            data['h_index'] = int((line.split('#hi')[1]).strip())\n",
    "        elif line.startswith( '#pi' ):\n",
    "            data['pa_index'] = float((line.split('#pi')[1]).strip())\n",
    "        elif line.startswith( '#upi' ):\n",
    "            data['p_index'] = float((line.split('#upi')[1]).strip())\n",
    "        elif line.startswith( '#t' ):\n",
    "            data['area'] = [a.strip() for a in (line.split('#t')[1]).split(';')]\n",
    "        else:\n",
    "            if not line or line == \"\":\n",
    "                if not data.keys():\n",
    "                    data = {}\n",
    "                else:\n",
    "                    author_corpus.append(data)\n",
    "                    data = {}\n",
    "\n",
    "    f.close()\n",
    "    print(\"processing successfully started\")\n",
    "    author_corpus = pd.DataFrame(author_corpus)\n",
    "    author_corpus = author_corpus[author_corpus['aid'].isin(author_to_paper['aid'])]\n",
    "    #author_corpus.to_json(folder_path+'author-corpus.json', orient='records', lines=True)\n",
    "    print(\"junior authors\", len(author_corpus))\n",
    "    print(\"starts\")\n",
    "    author_with_affiliation = pd.DataFrame([row for index, row in author_corpus.iterrows() if len(row['affiliations']) == 1 and row['affiliations'][0] != ''])\n",
    "    aid_without_affiliation = author_corpus[~(author_corpus['aid']).isin(author_with_affiliation['aid'])]['aid']\n",
    "    author_with_affiliation = author_with_affiliation[~(author_with_affiliation['aid']).isin(senior_author['aid'])]\n",
    "    print(\"author_with_affiliation starts\")\n",
    "    author_with_affiliation.to_json(folder_path+'author-corpus.json', orient='records', lines=True)\n",
    "    aid_without_affiliation.to_csv(folder_path+'aid_without_affi.csv', sep=',', encoding='utf-8', index=False)\n",
    "    print(\"remove authors with affic\", len(author_with_affiliation))\n",
    "    print(\"remove authors without affic\", len(aid_without_affiliation))\n",
    "    print(\"authors processing successfully done and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Remove authors link and papers, who havn't affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aid_without_affi = pd.read_csv(folder_path+'aid_without_affi.csv', sep=',', names=['aid_without_affi'])\n",
    "#author_to_paper = pd.read_json(folder_path+'author-2-paper.json', orient='records', encoding='utf8', lines=True)\n",
    "#rs_corpus = pd.read_json(folder_path+'rs-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "aid_without_affiction = list(aid_without_affi['aid_without_affi'])\n",
    "print(\"initial rs_corpus\", len(rs_corpus))\n",
    "print(\"author_to_paper\", len(author_to_paper))\n",
    "print(\"started\")\n",
    "rs_corpus_pid = list(set(author_to_paper.loc[(author_to_paper['aid'].isin(aid_without_affiction)) & (author_to_paper[\"pos\"] == 1)]['pid'].values))\n",
    "rs_corpus = rs_corpus[~rs_corpus['id'].isin(rs_corpus_pid)]\n",
    "author_to_paper = author_to_paper[~author_to_paper['pid'].isin(rs_corpus_pid)]\n",
    "author_to_paper.to_json(folder_path+'author-2-paper.json', orient='records', lines=True)\n",
    "rs_corpus.to_json(folder_path+'rs-corpus.json', orient='records', lines=True)\n",
    "print(\"after romoving author_to_paper\", len(author_to_paper))\n",
    "print(\"after romoving rs_corpus\", len(rs_corpus))\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on Author-2-Paper corpus\n",
    "#### -> count Authors papers (papers>2 or papers=<2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_to_paper = pd.read_json(folder_path+'author-2-paper.json', orient='records', encoding='utf8', lines=True)\n",
    "author_to_paper_groupby = author_to_paper.groupby(\"aid\")\n",
    "\n",
    "author_less_than_3_papers = []\n",
    "for index, row in author_to_paper_groupby:\n",
    "    data = row[(row['pos'] == 1) & (len(row['pos'] == 1)<3)]\n",
    "    if len(data['aid'])>0:\n",
    "        for index, rec in data.iterrows():\n",
    "            author_less_than_3_papers.append(list(rec))\n",
    "author_less_than_3_papers = pd.DataFrame(author_less_than_3_papers, columns = ['aid','pid','pos'])\n",
    "author_less_than_3_papers.to_csv(folder_path+'author_less_than_3_papers.csv', sep=',', encoding='utf-8', index=False)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> remove authors and links, who have less than 2 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#author_less_than_3_papers = pd.read_csv(folder_path+'author_less_than_3_papers.csv', sep=',', header=0, index_col=False)\n",
    "#author_to_paper = pd.read_json(folder_path+'author-2-paper.json', orient='records', encoding='utf8', lines=True)\n",
    "author_corpus = pd.read_json(folder_path+'author-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "rs_corpus = pd.read_json(folder_path+'rs-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "rs_aid = list(set(author_less_than_3_papers['aid']))\n",
    "rs_pid = list(set(author_less_than_3_papers['pid']))\n",
    "author_corpus = author_corpus[~author_corpus['aid'].isin(rs_aid)]\n",
    "author_to_paper = author_to_paper[~author_to_paper['pid'].isin(rs_pid)]\n",
    "rs_corpus = rs_corpus[~rs_corpus['id'].isin(rs_pid)]\n",
    "author_corpus.to_json(folder_path+'author-corpus.json', orient='records', lines=True)\n",
    "author_to_paper.to_json(folder_path+'author-2-paper.json', orient='records', lines=True)\n",
    "rs_corpus.to_json(folder_path+'rs-corpus.json', orient='records', lines=True)\n",
    "print(\"success\")\n",
    "print(len(author_corpus))\n",
    "print(len(author_to_paper))\n",
    "print(len(rs_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = author_to_paper[author_to_paper.pos==1]['pid'].values\n",
    "author_id = author_to_paper[author_to_paper.pos==1]['aid'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_corpus = rs_corpus[rs_corpus.id.isin(paper_id)]\n",
    "rs_corpus = pd.read_json(folder_path+'rs-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = author_to_paper[author_to_paper.pos==1]['pid'].values\n",
    "author_id = author_to_paper[author_to_paper.pos==1]['aid'].values\n",
    "author_corpus = pd.read_json(folder_path+'author-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "rs_corpus = pd.read_json(folder_path+'rs-corpus.json', orient='records', encoding='utf8', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(author_to_paper))\n",
    "author_to_paper = author_to_paper[author_to_paper['pid'].isin(rs_corpus['id'])]\n",
    "author_to_paper.to_json(folder_path+'author-2-paper.json', orient='records', lines=True)\n",
    "print(len(author_to_paper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(author_corpus))\n",
    "print(len(rs_corpus))\n",
    "author_corpus = author_corpus[author_corpus['aid'].isin(author_id)]\n",
    "rs_corpus = rs_corpus[rs_corpus['id'].isin(paper_id)]\n",
    "author_corpus.to_json(folder_path+'author-corpus.json', orient='records', lines=True)\n",
    "rs_corpus.to_json(folder_path+'rs-corpus.json', orient='records', lines=True)\n",
    "print(len(author_corpus))\n",
    "print(len(rs_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid = list(set(author_corpus['aid']))\n",
    "author_pid = author_to_paper[author_to_paper['aid'].isin(aid)]['pid'].values\n",
    "author_pid = list(set(author_pid))\n",
    "authors = author_to_paper[author_to_paper['pid'].isin(author_pid)]\n",
    "authors.to_json(folder_path+'author-2-paper.json', orient='records', lines=True)\n",
    "rscorpus = rs_corpus[rs_corpus['id'].isin(author_pid)]\n",
    "rscorpus.to_json(folder_path+'rs-corpus.json', orient='records', lines=True)\n",
    "print(len(set(aid)))\n",
    "print(len(author_pid))\n",
    "print(len(authors))\n",
    "print(len(rscorpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(author_corpus))\n",
    "author_id = author_to_paper[author_to_paper.pos==1]['aid'].values\n",
    "author_corpus = author_corpus[author_corpus['aid'].isin(author_id)]\n",
    "author_corpus.to_json(folder_path+'author-corpus.json', orient='records', lines=True)\n",
    "print(len(author_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_to_paper = pd.read_json(folder_path+'author-2-paper.json', orient='records', encoding='utf8', lines=True)\n",
    "author_to_paper = author_to_paper.groupby(\"aid\")\n",
    "author_to_paper_more_2 = author_to_paper.filter(lambda x: len(x) > 2)\n",
    "author_to_paper_less_2 = author_to_paper.filter(lambda x: len(x) < 3)\n",
    "author_to_paper_more_2.to_json(folder_path+'author-2-paper.json', orient='records', lines=True)\n",
    "author_to_paper_less_2.to_json(folder_path+'author-2-paper-less.json', orient='records', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_corpus = pd.read_json(folder_path+'rs-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "rs_corpus = rs_corpus[~rs_corpus['id'].isin(author_to_paper_less_2['pid'])]\n",
    "rs_corpus.to_json(folder_path+'rs-corpus.json', orient='records', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_to_paper = pd.read_json(folder_path+'author-2-paper.json', orient='records', encoding='utf8', lines=True)\n",
    "author_corpus = pd.read_json(folder_path+'author-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "author_corpus = author_corpus[author_corpus['aid'].isin(author_to_paper['aid'])]\n",
    "author_corpus.to_json(folder_path+'author-corpus.json', orient='records', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_corpus = pd.read_json(folder_path+'author-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "author_corpus = author_corpus[li]\n",
    "author_corpus.to_json(folder_path+'author-corpus.json', orient='records', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_to_paper = pd.read_json(folder_path+'author-2-paper.json', orient='records', encoding='utf8', lines=True)\n",
    "author_to_paper_2 = author_to_paper[~author_to_paper['aid'].isin(author_corpus['aid'])]\n",
    "author_to_paper = author_to_paper[author_to_paper['aid'].isin(author_corpus['aid'])]\n",
    "author_to_paper.to_json(folder_path+'author-2-paper.json', orient='records', lines=True)\n",
    "print(len(author_to_paper_2))\n",
    "print(len(author_to_paper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_corpus = pd.read_json(folder_path+'rs-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "rs_corpus = rs_corpus[~rs_corpus['id'].isin(author_to_paper_2['pid'])]\n",
    "rs_corpus.to_json(folder_path+'rs-corpus.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rs_corpus))\n",
    "print(len(set(author_to_paper['aid'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data\n",
    "#### -> junior-authors-more-2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_authors_more_2 = pd.read_csv(folder_path+'junior-authors-more-2.csv', sep=',', header=0, index_col=False)\n",
    "junior_authors_more_2 = junior_authors_more_2['junior_authors'].sort_values()\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> junior-authors-less-2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_authors_less_2 = pd.read_csv(folder_path+'junior-authors-less-2.csv', sep=',', header=0, index_col=False)\n",
    "junior_authors_less_2 = junior_authors_less_2['junior_authors'].sort_values()\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> junior-authors.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_authors = pd.read_csv(folder_path+'junior-authors-corpus.csv', sep=',', header=0, index_col=False)\n",
    "junior_authors = junior_authors['junior_authors'].sort_values()\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> author-2-paper.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_paper = pd.read_json(folder_path+'author-2-paper.json', orient='records', encoding='utf8', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_corpus = pd.read_json(folder_path+'rs-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "print(\"success\")\n",
    "print(len(rs_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = author_paper[author_paper['pid'].isin(junior_authors_less_2)]\n",
    "print(\"success\")\n",
    "print(len(paper_id['pid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rs_corpus = rs_corpus[~rs_corpus['id'].isin(paper_id['pid'])]\n",
    "new_rs_corpus.to_json(folder_path+'rs-corpus.json', orient='records', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation on author corpus\n",
    "#### -> read author-corpus.json\n",
    "#### -> delete authors in author-corpus, who have less-2 publications\n",
    "#### -> update author-corpus with more-2 publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_corpus = pd.read_json(folder_path+'author-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_authors_more_2 = author_corpus[author_corpus['aid'].isin(junior_authors)]\n",
    "junior_authors_more_2.to_json(folder_path+'author-corpus.json', orient='records', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_corpus = pd.read_json(folder_path+'vs_corpus_stdev.json', orient='records', encoding='utf8', lines=True)\n",
    "#grouped_vs_corpus = vs_corpus.groupby(\"year\")\n",
    "#grouped_vs_corpus.to_json(folder_path+'vs-corpus2.json', orient='records', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [v.id for k, v in vs_corpus.iterrows() if len(v.title) < 3]\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_corpus = vs_corpus[~vs_corpus.id.isin(ids)]\n",
    "vs_corpus.to_json(folder_path+'vs_corpus_stdev.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, v in grouped_vs_corpus:\n",
    "    print(index)\n",
    "    print(v.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on RS corpus & pre-processed corpus\n",
    "#### -> sort dataset by year\n",
    "#### -> save dataset into pre-processed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pre-processed json data again if needed\n",
    "file_preprocessed = open(folder_path+'vs-preprocessed-corpus.json', 'w+', encoding=\"utf8\")\n",
    "\n",
    "data_set = {}\n",
    "def add_doc_to_preprocessed(year, preprocessed_data):\n",
    "    data_set[str(year)] = preprocessed_data\n",
    "    data = {}\n",
    "    data[str(year)] = preprocessed_data\n",
    "    file_preprocessed.write((json.dumps(data)+'\\n'))\n",
    "    \n",
    "data_set5 = []\n",
    "data_set6 = []\n",
    "data_set7 = []\n",
    "data_set8 = []\n",
    "data_set9 = []\n",
    "count = 0\n",
    "with open(folder_path+'vs_corpus_avg.json','r', encoding=\"utf8\") as f:\n",
    "    for doc in f:\n",
    "        count += 1\n",
    "        if count % 2500 == 0:\n",
    "            print(count, \"doc processed\")\n",
    "        row = json.loads(doc)\n",
    "        pub_year = int(row['year'])\n",
    "        data = {}\n",
    "        data['id'] = row['id']\n",
    "        data['year'] = pub_year\n",
    "        data['title'] = row['title']\n",
    "        #data['abstract'] = row['abstract']\n",
    "        if pub_year == 2010:\n",
    "            data_set5.append(data)\n",
    "        elif pub_year == 2011:\n",
    "            data_set6.append(data)\n",
    "        elif pub_year == 2012:\n",
    "            data_set7.append(data)\n",
    "        elif pub_year == 2013:\n",
    "            data_set8.append(data)\n",
    "        elif pub_year == 2014:\n",
    "            data_set9.append(data)\n",
    "\n",
    "\n",
    "    print(\"pre-processed corpus loading start\")\n",
    "    add_doc_to_preprocessed(2010, data_set5)\n",
    "    print('5',len(data_set5))\n",
    "    add_doc_to_preprocessed(2011, data_set6)\n",
    "    print('6',len(data_set6))\n",
    "    add_doc_to_preprocessed(2012, data_set7)\n",
    "    print('7',len(data_set7))\n",
    "    add_doc_to_preprocessed(2013, data_set8)\n",
    "    print('8',len(data_set8))\n",
    "    add_doc_to_preprocessed(2014, data_set9)\n",
    "    print('9',len(data_set9))\n",
    "    file_preprocessed.close()\n",
    "    print(\"pre-processed corpus successfully loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent terms remove\n",
    "#### -> select specific year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select specific year data for LDA topic modeling, set year = ? e.g. 2005\n",
    "pub_year = 2009\n",
    "for year in data_set:\n",
    "    if int(year) == pub_year:\n",
    "        doc_year = data_set[str(year)]\n",
    "        print(year,\"corpus successfully selected\")\n",
    "        break\n",
    "        print(bcolors.STEXT+bcolors.HEADER+\"check common tokens, then run next 2 cells\"+bcolors.ENDC+bcolors.EETEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Count terms frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count tokens to list\n",
    "count = Counter(word for row in doc_year for word in row['title'])\n",
    "print(\"Common term successfully counted...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> select most common terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select most common terms for removal\n",
    "#31\n",
    "num_words = 50\n",
    "most_common_words = count.most_common(num_words)\n",
    "#print(bcolors.STEXT+bcolors.HEADER+\"remove common tokens\"+bcolors.ENDC+bcolors.ETEXT)\n",
    "\n",
    "if len(most_common_words)>0:\n",
    "    df = pd.DataFrame(most_common_words, columns = ['Word', 'Count'])\n",
    "    df.plot.bar(x='Word',y='Count')\n",
    "    plt.grid(axis = 'y', color ='white', linestyle='-')\n",
    "\n",
    "for w in range(num_words):\n",
    "    print(w, most_common_words[w])\n",
    "\n",
    "# 2005 index 0,1,2,5,6,7,8,10,11,12,13,15,17,21,22,23,24,29,38,39,49\n",
    "# 2006 index 0,2,3,4,5,6,7,10,11,12,13,14,17,19,23,29,31,34,35,45,47,48,49\n",
    "# 2007 index 0,1,2,3,5,6,7,8,11,12,13,14,15,16,17,22,23,24,34,37,39,40,41,44,45,49\n",
    "# 2008 index 0,1,2,3,5,6,7,8,9,10,11,12,15,16,23,26,30,35,36,38,38,39,40,41,42,46,47,48\n",
    "# 2009 index 0,1,2,3,5,6,7,10,11,12,13,16,23,24,28,29,31,33,37,40,41,43,44,46,47\n",
    "\n",
    "# 2005 index 0,1,2,3,4,7,8,9,10,11,12,13,15,16,17,18,19,20,21\n",
    "# 2006 index 0,2,3,4,5,6,10,11,12,13,14,15,16,19,20,21,22,24,25,28,38,31,33,34,35,37\n",
    "# 2007 index 0,2,3,4,5,6,7,8,11,12,13,14,17,20,21,22,25,26,27,29,30,31,33,34,37,39,40,42,43,44,45,46,47,49\n",
    "# 2008 index 0,1,3,6,7,9,10,11,13,14,16,19,21,22,23,25,26,30,31,32,35,36,38,39,40,42,45,46\n",
    "# 2009 index 0,1,4,7,9,10,12,13,17,18,19,20,22,24,25,27,29,30,32,33,34,36,41,42,43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> remove most common terms\n",
    "#### -> update pre-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove common terms form dataset\n",
    "# please run it carefully, it will remove data\n",
    "common_index = [0,1,4,7,9,10,12,13,17,18,19,20,22,24,25,27,29,30,32,33,34,36,41,42,43]\n",
    "print('common_index:',len(common_index))\n",
    "common_word_list = [ most_common_words[i][0] for i in common_index ]\n",
    "\n",
    "dummy_text = []\n",
    "texts = []\n",
    "# remove most common words\n",
    "for row in doc_year:\n",
    "    term_list = [t for t in row['title'] if t not in common_word_list]\n",
    "    # add year and tokens in list\n",
    "    data = {}\n",
    "    data['id'] = row['id']\n",
    "    data['year'] = row['year']\n",
    "    data['title'] = term_list\n",
    "    #data['abstract'] = row['abstract']\n",
    "    dummy_text.append(data)\n",
    "    texts.append(term_list)\n",
    "\n",
    "doc_year.clear()\n",
    "doc_year = dummy_text\n",
    "data_set[str(pub_year)] = dummy_text\n",
    "\n",
    "print(\"Common terms successfully removed from corpus\",pub_year,\"...\")\n",
    "#print(bcolors.STEXT+bcolors.HEADER+\"save new dataset into sorted file\"+bcolors.ENDC+bcolors.ETEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store sorted data into file\n",
    "with open(folder_path+'preprocessed-corpus.json', 'w+', encoding='utf8') as outfile:\n",
    "    for year in data_set:\n",
    "        data = {}\n",
    "        data[str(year)] = data_set[year]\n",
    "        outfile.write(json.dumps(data)+'\\n')\n",
    "    outfile.close()\n",
    "    print(\"pre-processed data has been updated\")\n",
    "#    print(bcolors.STEXT+bcolors.HEADER+\"models topics\"+bcolors.ENDC+bcolors.ETEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_to_paper = pd.read_json(folder_path+'author-2-paper.json', orient='records', encoding='utf8', lines=True)\n",
    "author_to_paper_pid_grouped = author_to_paper.groupby(\"pid\")\n",
    "unique_author = list(set(author_to_paper['aid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_score = []\n",
    "for first_auth in unique_author:\n",
    "    for second_auth in unique_author:\n",
    "        if first_auth != second_auth:\n",
    "            data = {}\n",
    "            ac_lk = 0.0\n",
    "            pac_k = 0.0\n",
    "            for pid, doc in author_to_paper_pid_grouped:\n",
    "                aid = list(doc['aid'])\n",
    "                if first_auth in aid and second_auth in aid:\n",
    "                    ac_lk += (1/int(doc[doc.aid == first_auth]['pos']))\n",
    "                    ac_lk += (1/int(doc[doc.aid == second_auth]['pos']))\n",
    "                if second_auth in aid:\n",
    "                    pac_k += (1/int(doc[doc.aid == second_auth]['pos']))\n",
    "\n",
    "            data['first_auth'] = first_auth\n",
    "            data['second_auth'] = second_auth\n",
    "            data['acw'] = ac_lk/pac_k\n",
    "            author_score.append(data)\n",
    "            \n",
    "pd.DataFrame(author_score).to_json(folder_path+'acw.json', orient='records', lines=True)\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = pd.read_json(folder_path+'HSRank/author-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "author_corpus = pd.read_csv(folder_path+'author-corpus.csv', sep=',', names=['aid', 'author', 'affiliation', 'n_paper', 'h_index', 'n_citation'])\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_authors = pd.read_json(folder_path+'author-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "author_corpus = pd.read_csv(folder_path+'author-info.csv', names=['aid', 'n_paper', 'h_index', 'n_citation'])\n",
    "author_corpus.drop_duplicates(keep='first', inplace=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_author = []\n",
    "c = 0\n",
    "for index, auth in hs_authors.iterrows():\n",
    "    row = author_corpus[author_corpus.aid == auth.aid]\n",
    "\n",
    "    if int(auth.aid) == int(row.aid):\n",
    "\n",
    "        if int(row.n_paper) > int(auth.n_paper):\n",
    "            auth.n_paper = int(row.n_paper)\n",
    "            c += 1\n",
    "\n",
    "        if int(row.h_index) > int(auth.h_index):\n",
    "            auth.h_index = int(row.h_index)\n",
    "            c += 1\n",
    "\n",
    "        if int(row.n_citation) > int(auth.n_citation):\n",
    "            auth.n_citation = int(row.n_citation)\n",
    "            c += 1\n",
    "\n",
    "    hs_author.append(auth)\n",
    "\n",
    "print(c)\n",
    "hs_author = pd.DataFrame(hs_author)\n",
    "hs_author.to_json(folder_path+'hs-author-corpus.json', orient='records', lines=True)\n",
    "hs_author.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_info = []\n",
    "with open(folder_path+'author-corpus.csv', 'r', encoding='utf8') as f:\n",
    "    for doc in f:\n",
    "        row = [x.strip() for x in doc.split(',')]\n",
    "        row_len = len(row)\n",
    "        author_info.append([row[0], row[row_len-3], row[row_len-2], row[row_len-1]])\n",
    "    pd.DataFrame(author_info).to_csv(folder_path+'author-info.csv', sep=',', encoding='utf-8', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coauthor_corpus[coauthor_corpus.aid.isin(star_author.aid.head(50))].cid.nunique())\n",
    "print(coauthor_corpus[coauthor_corpus.aid.isin(wmi_author.aid.head(50))].cid.nunique())\n",
    "print(coauthor_corpus[coauthor_corpus.aid.isin(hs_author.aid.head(50))].cid.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _thread\n",
    "import time\n",
    "\n",
    "# Define a function for the thread\n",
    "def print_time( threadName, delay):\n",
    "    count = 0\n",
    "    while count < 5:\n",
    "        time.sleep(delay)\n",
    "        count += 1\n",
    "        print (\"%s: %s\" % ( threadName, time.ctime(time.time()) ))\n",
    "\n",
    "# Create two threads as follows\n",
    "try:\n",
    "    _thread.start_new_thread( print_time, (\"Thread-1\", 2, ) )\n",
    "    _thread.start_new_thread( print_time, (\"Thread-2\", 4, ) )\n",
    "except:\n",
    "    print (\"Error: unable to start thread\")\n",
    "\n",
    "while 1:\n",
    "    pass\n",
    "    print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(min_df=1)\n",
    "tfidf = vect.fit_transform([\"I'd like an apple\", \"An apple a day keeps the doctor away\", \"Never compare an apple to an orange\",\n",
    "                             \"I prefer scikit-learn to Orange\"])\n",
    "(tfidf * tfidf.T).A\n",
    "print (cosine_sim('a little bird', 'a little bird'))\n",
    "print (cosine_sim('a little bird', 'a little bird chirps'))\n",
    "print (cosine_sim('a little bird', 'a big dog barks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#nlp = spacy.load('en')\n",
    "doc1 = nlp('Hello hi there!')\n",
    "doc2 = nlp('Hello hi there!')\n",
    "doc3 = nlp('Hey whatsup?')\n",
    "\n",
    "print(doc1.similarity(doc2)) # 0.999999954642\n",
    "print(doc2.similarity(doc3))  # 0.699032527716\n",
    "print(doc1.similarity(doc3))  # 0.699032527716"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLD Smimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate papers and topics sentence similarity score\n",
    "\n",
    "#store sorted data into file\n",
    "\n",
    "outfile_paper_similarity = open(folder_path+'vs-topic-paper-similarity-corpus.csv', 'w', newline='')\n",
    "paper_sim_writer = csv.writer(outfile_paper_similarity, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "def write_paper_similarity(paper_rows):\n",
    "    paper_sim_writer.writerows(paper_rows)\n",
    "    outfile_paper_similarity.flush()\n",
    "    \n",
    "def sentence_similarity(synsets1, synsets2, synsets1_technical, synsets2_technical):\n",
    "    \n",
    "    paper_score = 1/len(synsets2+synsets2_technical)\n",
    "    score = 0.0\n",
    "    \n",
    "    for itech in synsets2_technical:\n",
    "        if itech in synsets1_technical:\n",
    "            score += (paper_score+paper_score)\n",
    "    \n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets2:\n",
    "\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        best_score = [synset.path_similarity(ss) for ss in synsets1 if synset.path_similarity(ss) is not None]\n",
    "\n",
    "        if len(best_score)>=1:\n",
    "            if max(best_score)>0.49:\n",
    "                score += paper_score\n",
    "                \n",
    "    return score\n",
    "\n",
    "def get_topic_similarity(topic):\n",
    "    #get year only e.g. 2005, 2006, 2007, 2008 and 2009\n",
    "    topic_year = {}\n",
    "    for year in syn_data_set:\n",
    "        topic_score, paper_count = 0.0, 0.0\n",
    "        clear_output()\n",
    "        print(topic['id'],\" : \",year)\n",
    "        intyear = int(year)\n",
    "        topic_paper_similarity = []\n",
    "        for row in syn_data_set[year]:\n",
    "            \n",
    "            synsets1 = topic['syn']\n",
    "            synsets2 = row['syn']\n",
    "            synsets1_technical = topic['tech']\n",
    "            synsets2_technical = row['tech']\n",
    "            \n",
    "            paper_score = 1/len(synsets2+synsets2_technical)\n",
    "            score = 0.0\n",
    "\n",
    "            for itech in synsets2_technical:\n",
    "                if itech in synsets1_technical:\n",
    "                    score += (paper_score+paper_score)\n",
    "\n",
    "            # For each word in the first sentence\n",
    "            for synset in synsets2:\n",
    "\n",
    "                # Get the similarity value of the most similar word in the other sentence\n",
    "                best_score = [synset.path_similarity(ss) for ss in synsets1 if synset.path_similarity(ss) is not None]\n",
    "\n",
    "                if len(best_score)>=1:\n",
    "                    if max(best_score)>0.49:\n",
    "                        score += paper_score\n",
    "            \n",
    "            #sim_score = sentence_similarity(topic['syn'], row['syn'], topic['tech'], row['tech'])\n",
    "            data = []\n",
    "            data.append(topic['id'])\n",
    "            data.append(row['id'])\n",
    "            data.append(intyear)\n",
    "            data.append(score)\n",
    "            topic_paper_similarity.append(data)\n",
    "            #write_paper_similarity(data)\n",
    "        #_thread.start_new_thread(write_paper_similarity, (topic_paper_similarity,))\n",
    "        paper_sim_writer.writerows(topic_paper_similarity)\n",
    "        outfile_paper_similarity.flush()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #pool = ThreadPool(16)\n",
    "    print(\"similarity preprocessing has been started...\")\n",
    "    #pool.starmap(get_topic_similarity, zip(syn_lda_topic))\n",
    "    for topic in syn_lda_topic:\n",
    "        _thread.start_new_thread( get_topic_similarity, (topic, ) )\n",
    "    #pool.close() \n",
    "    #pool.join()\n",
    "    print(\"similarity has been successfully measured and saved\")\n",
    "    print(bcolors.STEXT+bcolors.HEADER+\"reload sorted dataset\"+bcolors.ENDC+bcolors.ETEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    "    #elif wn_tag is 'v':\n",
    "        #return ''\n",
    " \n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_sentence_synset(doc):\n",
    "    # Tokenize and tag\n",
    "    sentence = pos_tag(doc[topic_selection])\n",
    "    \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets = [tagged_to_synset(*tagged_word) for tagged_word in sentence]\n",
    "    \n",
    "    synsets_index = [i for i in range(len(synsets)) if synsets[i] is None]\n",
    "    synsets_technical = [sentence[ip][0] for ip in synsets_index]\n",
    "    \n",
    "    # Filter out the Nones\n",
    "    synsets = [ss for ss in synsets if ss]\n",
    "    \n",
    "    data = {}\n",
    "    data['id'] = doc['id']\n",
    "    data['tech'] = synsets_technical\n",
    "    data['syn'] = synsets\n",
    "    return data\n",
    "\n",
    "syn_lda_topic = []\n",
    "syn_data_set = {}\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    pool = ThreadPool(16)\n",
    "    print(\"Synset preprocessing has been started...\")\n",
    "    syn_lda_topic = pool.starmap(get_sentence_synset, zip(lda_topics))\n",
    "    print('Topics completed...')\n",
    "    for year in data_set:\n",
    "        syn_data_set[str(year)] = pool.starmap(get_sentence_synset, zip(data_set[str(year)]))\n",
    "        print(year, 'completed...')\n",
    "    pool.close() \n",
    "    pool.join()\n",
    "    print(\"Synset has been successfully measured and saved...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_length = {}\n",
    "with open(folder_path+'vs-preprocessed-corpus.json', 'r', encoding='utf8') as f:\n",
    "    for doc in f:\n",
    "        data = json.loads(doc)\n",
    "        for year in data:\n",
    "            data_set_length[str(year)] = len(data[year])\n",
    "    f.close()\n",
    "    print (\"data set length successfully loaded...\")\n",
    "    \n",
    "df_topic_similarity = pd.read_csv(folder_path+'vs-topic-paper-similarity-corpus.csv', names=['tid', 'pid', 'year', 'sim'])\n",
    "grouped_topic_similarity = df_topic_similarity.groupby(['tid'])\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read LDA topics file if needs\n",
    "lda_topics = []\n",
    "with open(folder_path+'lda-title-topics_lst.json','r', encoding='utf8') as f:\n",
    "    for doc in f:\n",
    "        topic = json.loads(doc)\n",
    "        topic['title'] = nlp(topic['title'])\n",
    "        lda_topics.append(topic)\n",
    "    f.close()\n",
    "print(len(lda_topics), \"Topics successfully readed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_corpus_nlp = []\n",
    "def getNLP(i, row):\n",
    "    \n",
    "    row.title = nlp(row.title)\n",
    "    vs_corpus_nlp.append(row)\n",
    "    clear_output()\n",
    "    print(i)\n",
    "    \n",
    "vs_corpus = pd.read_json(folder_path+'vs_corpus_avg.json', orient='records', encoding='utf8', lines=True)    \n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    {executor.submit(getNLP, i, row): row for i, row in vs_corpus.iterrows()}\n",
    "    \n",
    "print(len(vs_corpus_nlp), \"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load('en')\n",
    "outfile_paper_similarity = open(folder_path+'vs-topic-paper-similarity-corpus.csv', 'w', newline='')\n",
    "paper_sim_writer = csv.writer(outfile_paper_similarity, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "def write_paper_similarity(paper_rows):\n",
    "    print(\"saving start\")\n",
    "    paper_sim_writer.writerows(paper_rows)\n",
    "    outfile_paper_similarity.flush()\n",
    "    print(\"saving end\")\n",
    "\n",
    "def get_topic_similarity(topic):\n",
    "    \n",
    "    c = 0\n",
    "    topic_paper_similarity = []\n",
    "    for row in vs_corpus_nlp:\n",
    "        \n",
    "        topic_paper_similarity.append([topic['id'], row['id'], row['year'], (topic['title'].similarity(row['title']))])\n",
    "        c += 1\n",
    "        clear_output()\n",
    "        print(c)\n",
    "\n",
    "        if c > 4999:\n",
    "            #write_paper_similarity(topic_paper_similarity)\n",
    "            threading.Thread(target=write_paper_similarity, args=(topic_paper_similarity,)).start() \n",
    "            topic_paper_similarity = []\n",
    "            c = 0\n",
    "\n",
    "    #write_paper_similarity(topic_paper_similarity)\n",
    "    threading.Thread(target=write_paper_similarity, args=(topic_paper_similarity,)).start() \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    pool = ThreadPool(4)\n",
    "    print(\"similarity preprocessing has been started...\")\n",
    "    pool.starmap(get_topic_similarity, zip(lda_topics))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncomplete_similarity_lst = pd.read_csv(folder_path+'complete_similarity.csv', names=['tid', 'pid', 'year', 'sim'])\n",
    "uncomplete_similarity_lst = uncomplete_similarity_lst.groupby(['tid'])\n",
    "topic_id = uncomplete_similarity_lst.groups.keys()\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load('en')\n",
    "outfile_paper_similarity = open(folder_path+'vs-topic-paper-similarity-corpus.csv', 'w', newline='')\n",
    "paper_sim_writer = csv.writer(outfile_paper_similarity, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "def write_paper_ucsimilarity(paper_rows):\n",
    "    print(\"saving start\")\n",
    "    paper_sim_writer.writerows(paper_rows)\n",
    "    outfile_paper_similarity.flush()\n",
    "    print(\"saving end\")\n",
    "\n",
    "def get_topic_ucsimilarity(tid):\n",
    "    \n",
    "    c = 0\n",
    "    topic_paper_similarity = []\n",
    "    for topic in lda_topics:\n",
    "        if topic['id'] == tid:\n",
    "            for row in vs_corpus_nlp:\n",
    "\n",
    "                topic_paper_similarity.append([topic['id'], row['id'], row['year'], (topic['title'].similarity(row['title']))])\n",
    "                c += 1\n",
    "                clear_output()\n",
    "                print(c)\n",
    "\n",
    "                if c > 4999:\n",
    "                    threading.Thread(target=write_paper_ucsimilarity, args=(topic_paper_similarity,)).start() \n",
    "                    topic_paper_similarity = []\n",
    "                    c = 0\n",
    "\n",
    "            threading.Thread(target=write_paper_ucsimilarity, args=(topic_paper_similarity,)).start()\n",
    "            break\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    pool = ThreadPool(4)\n",
    "    print(\"similarity preprocessing has been started...\")\n",
    "    pool.starmap(get_topic_similarity, zip(topic_id))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics = pd.DataFrame(lda_topics)\n",
    "lda_topics.head()\n",
    "\n",
    "pd.DataFrame(str(complete)).to_json(folder_path+'complete_topics2.json', orient='records', lines=True)\n",
    "uncomplete.to_json(folder_path+'uncomplete_topics3.json', orient='records', lines=True)\n",
    "\n",
    "complete = lda_topics[lda_topics.id.isin(complete_topic)].tid\n",
    "uncomplete = lda_topics[lda_topics.id.isin(df_topic['tid'])].tid\n",
    "print(\"success\")\n",
    "\n",
    "df_topic_similarity = pd.read_csv(folder_path+'complete_similarity.csv', names=['tid', 'pid', 'year', 'sim'])\n",
    "#df_topic_similarity.drop_duplicates(keep='first', inplace=True)\n",
    "#grouped_topic_similarity = df_topic_similarity.groupby(['tid'])\n",
    "print(\"success\")\n",
    "\n",
    "df_topic_similarity.drop_duplicates(subset=['tid', 'pid'], keep='first', inplace=True)\n",
    "grouped_topic_similarity = df_topic_similarity.groupby(['tid'])\n",
    "print(\"success\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
