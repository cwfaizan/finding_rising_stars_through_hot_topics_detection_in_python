{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://nlpforhackers.io/wordnet-sentence-similarity/\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import gensim\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import scipy.interpolate as interpolate\n",
    "from operator import itemgetter\n",
    "from scipy import stats\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "from gensim.models import LdaModel\n",
    "#from stop_words import get_stop_words\n",
    "from nltk import pos_tag\n",
    "from IPython.display import clear_output\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import concurrent.futures\n",
    "from nltk.corpus import stopwords as nltk_stop_words, wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from scipy.interpolate import spline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "folder_path = 'D:/MS CS/RS DATA/dataset/HSRank/'\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_data_set = pd.read_json(folder_path+'set_rs.json', orient='records', encoding='utf8', lines=True)\n",
    "group_rs_data_set = rs_data_set.groupby(\"year\")\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on topics\n",
    "#### -> LDA title 100 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate LDA model\n",
    "number_of_topics = 100\n",
    "number_of_words = 10\n",
    "\n",
    "texts = [t['title'] for k, t in rs_data_set.iterrows()]\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "ldamodel = LdaModel(corpus=corpus, num_topics=number_of_topics, id2word=dictionary)\n",
    "#ldatopics = ldamodel.show_topics(num_words=25, formatted=False)\n",
    "\n",
    "top_words_per_topic = []\n",
    "for t in range(ldamodel.num_topics):\n",
    "    top_words_per_topic.extend([(t, ) + x for x in ldamodel.show_topic(t, topn = number_of_words)])\n",
    "\n",
    "lda_topics = []\n",
    "for key, topic in itertools.groupby(top_words_per_topic, key=itemgetter(0)):\n",
    "    data = {}\n",
    "    data['id'] = 'Topic-'+str(key+1)\n",
    "    data['title'] = [t[1] for t in topic]\n",
    "    lda_topics.append(data)\n",
    "\n",
    "print(number_of_topics,\"Topics and\",number_of_words,\"words successfully generated...\")\n",
    "\n",
    "with open(folder_path+'lda-title-topics.json', 'w+', encoding='utf8') as outfile:\n",
    "    for lda_topic in lda_topics:\n",
    "        outfile.write(json.dumps(lda_topic)+'\\n')\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Papers 500 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate LDA model\n",
    "number_of_topics = 100\n",
    "number_of_words = 20\n",
    "lda_paper_topics = []\n",
    "for k, v in group_rs_data_set:\n",
    "\n",
    "    texts = [t['title'] for k, t in v.iterrows()]\n",
    "\n",
    "    # turn our tokenized documents into a id <-> term dictionary\n",
    "    dictionary = gensim.corpora.Dictionary(texts)\n",
    "\n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    ldamodel = LdaModel(corpus=corpus, num_topics=number_of_topics, id2word=dictionary)\n",
    "    #ldatopics = ldamodel.show_topics(num_words=25, formatted=False)\n",
    "\n",
    "    top_words_per_topic = []\n",
    "    for t in range(ldamodel.num_topics):\n",
    "        top_words_per_topic.extend([(t, ) + x for x in ldamodel.show_topic(t, topn = number_of_words)])\n",
    "\n",
    "    for key, topic in itertools.groupby(top_words_per_topic, key=itemgetter(0)):\n",
    "        data = {}\n",
    "        data['year'] = k\n",
    "        data['id'] = str(k)+'-'+str(key+1)\n",
    "        data['title'] = [t[1] for t in topic]\n",
    "        lda_paper_topics.append(data)\n",
    "\n",
    "    print(number_of_topics,\"Topics and\",number_of_words,\"words successfully generated...\")\n",
    "\n",
    "pd.DataFrame(lda_paper_topics).to_json(folder_path+'lda_paper_topics.json', orient='records', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read title-topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read LDA topics file if needs\n",
    "lda_topics = []\n",
    "with open(folder_path+'lda-title-topics.json','r', encoding='utf8') as f:\n",
    "    for doc in f:\n",
    "        lda_topics.append(json.loads(doc))\n",
    "    f.close()\n",
    "print(len(lda_topics), \"Topics successfully readed...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read papers-topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_paper_topics = []\n",
    "with open(folder_path+'lda_paper_topics.json','r', encoding='utf8') as f:\n",
    "    for doc in f:\n",
    "        lda_paper_topics.append(json.loads(doc))\n",
    "    f.close()\n",
    "print(len(lda_paper_topics), \"Topics successfully readed...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read set_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_corpus = []\n",
    "with open(folder_path+'set_rs.json','r', encoding='utf8') as f:\n",
    "    for doc in f:\n",
    "        rs_corpus.append(json.loads(doc))\n",
    "    f.close()\n",
    "print(len(rs_corpus), \"Topics successfully readed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_corpus = []\n",
    "with open(folder_path+'set_vs.json','r', encoding='utf8') as f:\n",
    "    for doc in f:\n",
    "        vs_corpus.append(json.loads(doc))\n",
    "    f.close()\n",
    "print(len(vs_corpus), \"Topics successfully readed...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following data processing operations are performed\n",
    "#### 1) Check similarity of topics\n",
    "#### 2) Get score of topics of each year e.g. 2005, 2006, 2007, 2008 and 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    "    #elif wn_tag is 'v':\n",
    "        #return ''\n",
    " \n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_sentence_synset(doc):\n",
    "    # Tokenize and tag\n",
    "    #sentence = pos_tag(tokenizer.tokenize(doc['title']))\n",
    "    sentence = pos_tag(doc['title'])\n",
    "    \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets = [tagged_to_synset(*tagged_word) for tagged_word in sentence]\n",
    "    \n",
    "    synsets_index = [i for i in range(len(synsets)) if synsets[i] is None]\n",
    "    synsets_technical = [sentence[ip][0] for ip in synsets_index]\n",
    "    \n",
    "    # Filter out the Nones\n",
    "    synsets = [ss for ss in synsets if ss]\n",
    "    \n",
    "    data = {}\n",
    "    data['id'] = doc['id']\n",
    "    data['tech'] = synsets_technical\n",
    "    data['syn'] = synsets\n",
    "    return data\n",
    "\n",
    "def get_sentence_synsets(doc):\n",
    "    # Tokenize and tag\n",
    "    #sentence = pos_tag(tokenizer.tokenize(doc['title']))\n",
    "    sentence = pos_tag(doc['title'])\n",
    "    \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets = [tagged_to_synset(*tagged_word) for tagged_word in sentence]\n",
    "    \n",
    "    synsets_index = [i for i in range(len(synsets)) if synsets[i] is None]\n",
    "    synsets_technical = [sentence[ip][0] for ip in synsets_index]\n",
    "    \n",
    "    # Filter out the Nones\n",
    "    synsets = [ss for ss in synsets if ss]\n",
    "    \n",
    "    data = {}\n",
    "    data['id'] = doc['id']\n",
    "    data['year'] = doc['year']\n",
    "    data['tech'] = synsets_technical\n",
    "    data['syn'] = synsets\n",
    "    return data\n",
    "\n",
    "syn_lda_topic = []\n",
    "syn_rs_set = []\n",
    "syn_vs_set = []\n",
    "syn_rs_set_paper = []\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    pool = ThreadPool(4)\n",
    "    print(\"Synset preprocessing has been started...\")\n",
    "    syn_lda_topic = pool.starmap(get_sentence_synset, zip(lda_topics))\n",
    "    print('Topics completed...')\n",
    "    syn_rs_set = pool.starmap(get_sentence_synsets, zip(rs_corpus))\n",
    "    #syn_vs_set = pool.starmap(get_sentence_synsets, zip(vs_corpus))\n",
    "    print('rs_corpus completed...')\n",
    "    #syn_rs_set_paper = pool.starmap(get_sentence_synsets, zip(lda_paper_topics))\n",
    "    print('lda_paper_topics completed...')\n",
    "    pool.close() \n",
    "    pool.join()\n",
    "    print(\"Synset has been successfully measured and saved...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lda_topics))\n",
    "print(len(syn_rs_set_paper))\n",
    "print(len(syn_rs_set))\n",
    "print(len(syn_vs_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_paper_similarity = open(folder_path+'paper-similarity-corpus.csv', 'w', newline='')\n",
    "#outfile_paper_similarity = open(folder_path+'topic-similarity-corpus.csv', 'w', newline='')\n",
    "#outfile_paper_similarity = open(folder_path+'vs-paper-similarity-corpus.csv', 'w', newline='')\n",
    "paper_sim_writer = csv.writer(outfile_paper_similarity, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "def write_paper_similarity(paper_rows):\n",
    "    paper_sim_writer.writerows(paper_rows)\n",
    "    outfile_paper_similarity.flush()\n",
    "    \n",
    "def get_topic_similarity(topic):\n",
    "    \n",
    "    c = 0\n",
    "    similarity_lst = []\n",
    "    #for row in syn_rs_set:\n",
    "    for row in syn_rs_set:\n",
    "        \n",
    "        synsets1 = topic['syn']\n",
    "        synsets2 = row['syn']\n",
    "        synsets1_technical = topic['tech']\n",
    "        synsets2_technical = row['tech']\n",
    "\n",
    "        score = 0.0\n",
    "        count = 0\n",
    "\n",
    "        for itech in synsets2_technical:\n",
    "            if itech in synsets1_technical:\n",
    "                score += 1\n",
    "                count += 1\n",
    "        \n",
    "        # For each word in the first sentence\n",
    "        for synset in synsets2:\n",
    "\n",
    "            # Get the similarity value of the most similar word in the other sentence\n",
    "            best_score = 0.0\n",
    "            for ss in synsets1:\n",
    "                b_score = synset.path_similarity(ss)\n",
    "                if b_score:\n",
    "                    if b_score>best_score:\n",
    "                        best_score = b_score\n",
    "\n",
    "            if best_score>0.0:\n",
    "                score += best_score\n",
    "                count += 1\n",
    "                \n",
    "        if count == 0:\n",
    "            count = 1\n",
    "        similarity_lst.append([topic['id'], row['id'], row['year'], (score/count)])\n",
    "        c += 1\n",
    "        clear_output()\n",
    "        print(c)\n",
    "\n",
    "        if c > 4999:\n",
    "            threading.Thread(target=write_paper_similarity, args=(similarity_lst,)).start() \n",
    "            similarity_lst.clear()\n",
    "            c = 0\n",
    "        \n",
    "    threading.Thread(target=write_paper_similarity, args=(similarity_lst,)).start()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #pool = ThreadPool(4)\n",
    "    print(\"similarity preprocessing has been started...\")\n",
    "    #pool.starmap(get_topic_similarity, zip(syn_lda_topic))\n",
    "    #pool.close() \n",
    "    #pool.join()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for row in syn_lda_topic:\n",
    "            executor.submit(get_topic_similarity, row)\n",
    "    print(\"similarity has been successfully measured and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following data processing operations are performed\n",
    "#### 1) Check topics is hot or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_similarity = pd.read_csv(folder_path+'topic-similarity-corpus.csv', names=['tid', 'pid', 'year', 'sim'])\n",
    "grouped_topic_similarity = df_topic_similarity.groupby(['tid'])\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create arrays of fake points\n",
    "my_xticks = ['2005', '2006', '2007', '2008', '2009']\n",
    "x = np.array([1, 2, 3,  4,  5])\n",
    "    \n",
    "# Polynomial Regression\n",
    "def polyfit(x, y, degree):\n",
    "    results = {}\n",
    "    \n",
    "    coeffs = np.polyfit(x, y, degree)\n",
    "\n",
    "     # Polynomial Coefficients\n",
    "    results['polynomial'] = coeffs.tolist()\n",
    "\n",
    "    # r-squared\n",
    "    p = np.poly1d(coeffs)\n",
    "    \n",
    "    # fit values, and mean\n",
    "    yhat = p(x)\n",
    "    ybar = np.sum(y)/len(y)\n",
    "    ssreg = np.sum((yhat-ybar)**2)\n",
    "    sstot = np.sum((y - ybar)**2)\n",
    "    results['p2_score'] = ssreg / sstot\n",
    "\n",
    "    return results\n",
    "\n",
    "#line smoothing\n",
    "def smoothing(x, deg_val, k_val = 2):\n",
    "    \n",
    "    y = np.polyval(deg_val,x)\n",
    "    x_smooth = np.linspace(x.min(), x.max(), 100)\n",
    "    tck = interpolate.splrep(x, y, k=k_val)\n",
    "    y_smooth = interpolate.splev(x_smooth, tck)\n",
    "\n",
    "    return x_smooth, y_smooth\n",
    "\n",
    "#for tid, data in grouped_year_similarity:\n",
    "hot_topic_id = []\n",
    "cold_topic_id = []\n",
    "#topic = []\n",
    "for tid, row_by_topic in grouped_topic_similarity:\n",
    "    \n",
    "    topic = row_by_topic.groupby(['year']).sim.mean().tolist()\n",
    "    \n",
    "    # fit up to deg=2\n",
    "    row = polyfit(x, topic, 2)\n",
    "    p2_score = row['p2_score']\n",
    "    deg2 = row['polynomial']\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,topic)\n",
    "    #r_value > 0.80\n",
    "    if p2_score > 0.90:\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.xticks(x, my_xticks)\n",
    "        topic_num = tid\n",
    "        x_smooth, y_smooth = smoothing(x, deg2)\n",
    "        min_val = min(topic)\n",
    "        if (min_val == topic[2]) and (topic[2]<topic[3]<topic[4]):\n",
    "            plt.plot(x, topic, marker='s', linestyle=':', color='r', label='Square')\n",
    "            plt.plot(x_smooth, y_smooth,'r-')\n",
    "            hot_topic_id.append(tid)\n",
    "        elif topic[0]<topic[2]<topic[3]<topic[4]:\n",
    "            plt.plot(x, topic, marker='s', linestyle=':', color='r', label='Square')\n",
    "            plt.plot(x_smooth, y_smooth,'r-')\n",
    "            hot_topic_id.append(tid)\n",
    "        elif (min_val == topic[1] or min_val == topic[2]) and (topic[2]<topic[3]<topic[4]):\n",
    "            plt.plot(x, topic, marker='s', linestyle=':', color='r', label='Square')\n",
    "            plt.plot(x_smooth, y_smooth,'r-')\n",
    "            hot_topic_id.append(tid)\n",
    "        elif deg2[0] < 0 and slope < 0:\n",
    "            plt.plot(x, topic, marker='s', linestyle=':', color='b', label='Square')\n",
    "            plt.plot(x_smooth, y_smooth,'b-')\n",
    "            cold_topic_id.append(tid)\n",
    "        elif deg2[0] < 0 and deg2[1] > 0 and r_value > 0.80:\n",
    "            plt.plot(x, topic, marker='s', linestyle=':', color='b', label='Square')\n",
    "            plt.plot(x_smooth, y_smooth,'b-')\n",
    "            cold_topic_id.append(tid)\n",
    "        elif r_value < 0.80:\n",
    "            plt.plot(x, topic, marker='s', linestyle=':', color='b', label='Square')\n",
    "            plt.plot(x_smooth, y_smooth,'b-')\n",
    "            cold_topic_id.append(tid)\n",
    "        else:\n",
    "            plt.plot(x, topic, marker='s', linestyle=':', color='r', label='Square')\n",
    "            plt.plot(x_smooth, y_smooth,'r-')\n",
    "            hot_topic_id.append(tid)\n",
    "            \n",
    "        #plt.xlabel('recognition span')\n",
    "        plt.ylabel('contribution')\n",
    "        plt.title(tid)\n",
    "    else:\n",
    "        plt.figure()\n",
    "        plt.xticks(x, my_xticks)\n",
    "        topic_num = tid\n",
    "        x_smooth, y_smooth = smoothing(x, deg2)\n",
    "        \n",
    "        plt.plot(x, topic, marker='s', linestyle=':', color='b', label='Square')\n",
    "        plt.plot(x_smooth, y_smooth,'b-')\n",
    "        cold_topic_id.append(tid)\n",
    "        \n",
    "        #plt.xlabel('recognition span')\n",
    "        plt.ylabel('contribution')\n",
    "        plt.title(tid)\n",
    "        \n",
    "hot_topic_id = pd.DataFrame(hot_topic_id)\n",
    "hot_topic_id.to_csv(folder_path+'hot_topic_id.csv', sep=',', encoding='utf-8', header=None, index=False)\n",
    "pd.DataFrame(cold_topic_id).to_csv(folder_path+'cold_topic_id.csv', sep=',', encoding='utf-8', header=None, index=False)\n",
    "print(\"Total number of Hot topics are\", len(hot_topic_id), \",all another topics can't fulfill criteria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select hot topics papers with their similarity and save it\n",
    "### calculate paper contribution in hot topics and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_topic_id = pd.read_csv(folder_path+'hot_topic_id.csv', names=['tid'])\n",
    "hot_topic_id = hot_topic_id['tid'].tolist()\n",
    "df_topic_similarity = pd.read_csv(folder_path+'paper-similarity-corpus.csv', names=['tid', 'pid', 'year', 'sim'])\n",
    "hot_topic_similarity = df_topic_similarity.loc[df_topic_similarity['tid'].isin(hot_topic_id)]\n",
    "hot_topic_similarity.to_csv(folder_path+'hot-topic-paper-similarity.csv', sep=',', encoding='utf-8', header=None, index=False)\n",
    "print(\"success\")\n",
    "\n",
    "hot_topic_pid = list(set(hot_topic_similarity['pid']))\n",
    "hot_topic_count = len(hot_topic_id)\n",
    "paper_contribution_in_ht = []\n",
    "for pid in hot_topic_pid:\n",
    "    paper_contribution_in_ht.append([pid, (sum(hot_topic_similarity[hot_topic_similarity['pid'] == pid]['sim'])/hot_topic_count)])\n",
    "paper_contribution_in_ht = pd.DataFrame(paper_contribution_in_ht)\n",
    "paper_contribution_in_ht.to_csv(folder_path+'paper-contribution-in-ht.csv', sep=',', encoding='utf-8', header=None, index=False)\n",
    "author_corpus_cite = pd.read_json(folder_path+'author_corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### author contribution in papers and rank it by calculating HS Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmi_aid = pd.read_csv('D:/MS CS/RS DATA/dataset/WMIRank/aid_wmirank.csv', names=['aid']).aid.tolist()\n",
    "author_corpus_cite = pd.read_json(folder_path+'author_corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "paper_contribution_in_ht = pd.read_csv(folder_path+'paper-contribution-in-ht.csv', names=['pid', 'score'])\n",
    "author_to_paper = pd.read_json(folder_path+'author_to_paper.json', orient='records', encoding='utf8', lines=True)\n",
    "\n",
    "with open(folder_path+'author_corpus.json', 'r', encoding='utf8') as f:\n",
    "    author_corpus = []\n",
    "    for doc in f:\n",
    "        data = json.loads(doc)\n",
    "        author_to_paper_pid = list(set(author_to_paper[author_to_paper['aid'] == data['aid']]['pid']))\n",
    "        author_to_paper_pid_count = len(author_to_paper_pid)\n",
    "        n_citation = int(author_corpus_cite[author_corpus_cite.aid == data['aid']].n_citation)\n",
    "        n_paper = int(author_corpus_cite[author_corpus_cite.aid == data['aid']].n_paper)\n",
    "        data['hs_score'] = sum(paper_contribution_in_ht[paper_contribution_in_ht['pid'].isin(author_to_paper_pid)]['score'])/author_to_paper_pid_count     \n",
    "        del data['p_index']\n",
    "        del data['pa_index']\n",
    "        author_corpus.append(data)\n",
    "    f.close()\n",
    "author_corpus = pd.DataFrame(author_corpus)\n",
    "author_corpus.sort_values(by=['hs_score'], inplace=True, ascending=False)\n",
    "author_corpus.to_json(folder_path+'hs-author-corpus.json', orient='records', lines=True)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0-1 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_corpus = pd.read_json(folder_path+'hs-author-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "minScore = author_corpus.hs_score.min()\n",
    "maxScore = author_corpus.hs_score.max()\n",
    "norm_corpus = []\n",
    "for i, rec in author_corpus.iterrows():\n",
    "    rec['n_score'] = (rec.hs_score - minScore) / (maxScore - minScore)\n",
    "    norm_corpus.append(rec)\n",
    "    \n",
    "pd.DataFrame(norm_corpus).to_json(folder_path+'hs-author-corpus.json', orient='records', lines=True)\n",
    "author_corpus = pd.read_json(folder_path+'hs-author-corpus.json', orient='records', encoding='utf8', lines=True)\n",
    "author_corpus.sort_values(by=['hs_score'], inplace=True, ascending=False)\n",
    "print(\"success\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
