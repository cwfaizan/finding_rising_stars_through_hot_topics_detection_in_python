{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import ijson\n",
    "import bisect\n",
    "import gensim\n",
    "import nltk\n",
    "import _thread\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import scipy.interpolate as interpolate\n",
    "from operator import itemgetter\n",
    "from scipy import stats\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "from gensim.models import LdaModel\n",
    "from stop_words import get_stop_words\n",
    "from nltk import pos_tag\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from nltk.corpus import stopwords as nltk_stop_words, wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from scipy.interpolate import spline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variables have initialized...\n"
     ]
    }
   ],
   "source": [
    "# path where files reside\n",
    "folder_path = 'D:/Python Workspace/Sir Tariq Ali/'\n",
    "#folder_path = 'D:/Python Workspace/Sir Tariq Ali/'\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "# Bring in the default English NLTK stop words\n",
    "en_stop += nltk_stop_words.words('english')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "print(\"All variables have initialized...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(folder_path+'data.csv', sep=',', names=['did', 'doc', 'cls'])\n",
    "file_enron = open(folder_path+'Enron-format.arff', 'w+', encoding=\"utf8\")\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>did</th>\n",
       "      <th>doc</th>\n",
       "      <th>cls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>'The system shall refresh the display every 60...</td>\n",
       "      <td>PE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>'The application shall match the color of the ...</td>\n",
       "      <td>LF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>' If projected  the data must be readable.  On...</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>' The product shall be available during normal...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>' If projected  the data must be understandabl...</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   did                                                doc cls\n",
       "0    1  'The system shall refresh the display every 60...  PE\n",
       "1    1  'The application shall match the color of the ...  LF\n",
       "2    1  ' If projected  the data must be readable.  On...  US\n",
       "3    1  ' The product shall be available during normal...   A\n",
       "4    1  ' If projected  the data must be understandabl...  US"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'F', 'FT', 'L', 'LF', 'MN', 'O', 'PE', 'PO', 'SC', 'SE', 'US']\n"
     ]
    }
   ],
   "source": [
    "sorted_cls = sorted(list(set(data['cls'])))\n",
    "print(sorted_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7356\n",
      "1060\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "stemmed_tokens = []\n",
    "stemmed_list = []\n",
    "def doc_preprocessing(doc):\n",
    "    # clean and tokenize document string\n",
    "    raw = (doc.lower().strip(':-;?!.,@')).strip()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if i not in en_stop]\n",
    "\n",
    "    # steme words from stopped_tokens\n",
    "    for i in stopped_tokens:\n",
    "        stm = p_stemmer.stem(i.strip())\n",
    "        if len(stm)>1 and re.match('^[a-zA-Z_]+$', stm) and not stm.isdigit():\n",
    "            stemmed_tokens.append(stm)\n",
    "    stemmed_list.append(sorted(stemmed_tokens))\n",
    "\n",
    "for doc in data['doc']:\n",
    "    doc_preprocessing(doc)\n",
    "\n",
    "print(len(stemmed_tokens))\n",
    "stemmed_tokens = sorted(list(set(stemmed_tokens)))\n",
    "print(len(stemmed_tokens))\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "file_enron.write('@relation nfr\\n\\n')\n",
    "for cls in sorted_cls:\n",
    "    file_enron.write('@attribute '+cls+' {0,1}\\n')\n",
    "    \n",
    "for token in stemmed_tokens:\n",
    "    file_enron.write('@attribute '+token+' numeric\\n')\n",
    "file_enron.write('\\n@data\\n')\n",
    "    \n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "for rec in stemmed_list:\n",
    "    row = '{'\n",
    "    for token in rec:\n",
    "        row += (str(stemmed_tokens.index(token)+1) + ' 1,')\n",
    "    row = row[:-1]+'}'\n",
    "    file_enron.write(row+'\\n')\n",
    "file_enron.close()\n",
    "print(\"success\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
